{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "UhbdAWmewbaJ"
      },
      "source": [
        "# Ajuste fino (Fine-Tunne) de um Modelo de IA Generativa para Sumarização de Diálogos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJDp_jkXwbaP"
      },
      "source": [
        "\n",
        "Neste notebook, você irá fazer o fine-tune em um LLM existente da Hugging Face para melhorar a sumarização de diálogos. Você utilizará o modelo [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) , que oferece um modelo ajustado de alta qualidade e capaz de sumarizar texto diretamente. Para aprimorar as inferências, você explorará uma abordagem completa de fine-tuning e avaliará os resultados com métricas ROUGE. Em seguida, você realizará o Parameter Efficient Fine-Tuning (PEFT), avaliará o modelo resultante e verá que os benefícios do PEFT superam ligeiramente as métricas de desempenho mais baixas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4J5U9QhwbaQ"
      },
      "source": [
        "# Table of Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "jw96Yd5wwbaR"
      },
      "source": [
        "- [ 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM](#1)\n",
        "  - [ 1.1 - Set up Kernel and Required Dependencies](#1.1)\n",
        "  - [ 1.2 - Load Dataset and LLM](#1.2)\n",
        "  - [ 1.3 - Test the Model with Zero Shot Inferencing](#1.3)\n",
        "- [ 2 - Perform Full Fine-Tuning](#2)\n",
        "  - [ 2.1 - Preprocess the Dialog-Summary Dataset](#2.1)\n",
        "  - [ 2.2 - Fine-Tune the Model with the Preprocessed Dataset](#2.2)\n",
        "  - [ 2.3 - Evaluate the Model Qualitatively (Human Evaluation)](#2.3)\n",
        "  - [ 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#2.4)\n",
        "- [ 3 - Perform Parameter Efficient Fine-Tuning (PEFT)](#3)\n",
        "  - [ 3.1 - Setup the PEFT/LoRA model for Fine-Tuning](#3.1)\n",
        "  - [ 3.2 - Train PEFT Adapter](#3.2)\n",
        "  - [ 3.3 - Evaluate the Model Qualitatively (Human Evaluation)](#3.3)\n",
        "  - [ 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#3.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqwrBTbswbaS"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Configure o Kernel, Carregue as Dependências Necessárias, Conjunto de Dados e LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cotJUphwbaS"
      },
      "source": [
        ":<a name='1.1'></a>\n",
        "### 1.1 - Set up Kernel and Required Dependencies (SOMENTE PARA SAGEMAKER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "6c0r-j9jwbaU",
        "outputId": "b12dcbd5-a7a2-4695-a117-ba72465f26b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expected instance type: instance-datascience-ml-m5-2xlarge\n",
            "Currently chosen instance type: instance-datascience-ml-m5-2xlarge\n",
            "Instance type has been chosen correctly.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "instance_type_expected = 'ml-m5-2xlarge'\n",
        "instance_type_current = os.environ.get('HOSTNAME')\n",
        "\n",
        "print(f'Expected instance type: instance-datascience-{instance_type_expected}')\n",
        "print(f'Currently chosen instance type: {instance_type_current}')\n",
        "\n",
        "assert instance_type_expected in instance_type_current, f'ERROR. You selected the {instance_type_current} instance type. Please select {instance_type_expected} instead as shown on the screenshot above'\n",
        "print(\"Instance type has been chosen correctly.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "eTO9uLJfwbaY",
        "outputId": "ff78908f-8177-493a-df82-a890476acd69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets==2.17.0 in /opt/conda/lib/python3.10/site-packages (2.17.0)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (1.26.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (14.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.3.7)\n",
            "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (2.1.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (4.64.1)\n",
            "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.0) (2023.10.0)\n",
            "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.22.2)\n",
            "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from datasets==2.17.0) (6.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.17.0) (3.0.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2022.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.0) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Instala a versão 2.17.0 da biblioteca datasets\n",
        "%pip install -U datasets==2.17.0\n",
        "\n",
        "# Atualiza a versão do pip\n",
        "%pip install --upgrade pip\n",
        "\n",
        "# Instalação silenciosa das versões específicas de torch e torchdata\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1 --quiet\n",
        "\n",
        "# Instalação silenciosa das bibliotecas especificadas\n",
        "%pip install \\\n",
        "    transformers==4.27.2 \\\n",
        "    evaluate==0.4.0 \\\n",
        "    rouge_score==0.1.2 \\\n",
        "    loralib==0.1.1 \\\n",
        "    peft==0.3.0 --quiet\n",
        "\n",
        "#-- quiet oculta algumas informações que seriam exibidas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "VjRUU8gwwbaa"
      },
      "source": [
        "Importe as bibliotecas necessárias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "GxPR-lOEwbaa"
      },
      "outputs": [],
      "source": [
        "#importando as bibliotecas\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
        "import torch\n",
        "import time\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "43GGxQebwbab"
      },
      "source": [
        "<a name='1.2'></a>\n",
        "### 1.2 - Carregando o Dataset e o LLM\n",
        "\n",
        "Carregando o conjunto de dados [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) da Hugging Face. Ele contém mais de 10.000 diálogos com os resumos manualmente rotulados correspondentes e tópicos.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "4bf3842b0c074912b1bb0852762b38b1",
            "0bc8ad43b28e496b9fae52bcf014307b",
            "ae8c0b5d1e1a4c518eca7c433ab44e80",
            "a6768fc86b8a43fda12f85add3c54cfe",
            "073958a9e80b4359be2b97e073471c4f",
            "c49939b5cc624ffc975c6eb7bbc60486",
            "43dbd91d0f4544eb9c745de289a9c01b"
          ]
        },
        "id": "BGoKxLNGwbac",
        "outputId": "da6470a2-f7a4-49de-e642-ed7dff1e6488"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bf3842b0c074912b1bb0852762b38b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bc8ad43b28e496b9fae52bcf014307b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae8c0b5d1e1a4c518eca7c433ab44e80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6768fc86b8a43fda12f85add3c54cfe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "073958a9e80b4359be2b97e073471c4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c49939b5cc624ffc975c6eb7bbc60486",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43dbd91d0f4544eb9c745de289a9c01b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 12460\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 1500\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "\n",
        "dataset = load_dataset(huggingface_dataset_name)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "oERzn1xawbac"
      },
      "source": [
        "Carregue o modelo pré-treinado [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5) e seu tokenizador diretamente da HuggingFace. Observe que você estará utilizando a versão pequena [small version](https://huggingface.co/google/flan-t5-base) do FLAN-T5. Definir `torch_dtype=torch.bfloat16` especifica o tipo de memória a ser usado por este modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "8ebe5fcefeec41c980332ff927260d69",
            "4b909a14998245c49ab05b0537f4f5c0",
            "f0ffb5455456407aad44cf105b2bfdde",
            "07c4ee2cfe2643cd991c20a822091d16",
            "d9db5478a91349669319d36a536e791c",
            "6debd3c095824a1696cd9e085246fede",
            "e3d64140271c4b04b52d6301cb25f7e8"
          ]
        },
        "id": "PEFeN3oJwbad",
        "outputId": "7692c11b-66aa-4eae-e863-c5fdb4cadab6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ebe5fcefeec41c980332ff927260d69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b909a14998245c49ab05b0537f4f5c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0ffb5455456407aad44cf105b2bfdde",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07c4ee2cfe2643cd991c20a822091d16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9db5478a91349669319d36a536e791c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6debd3c095824a1696cd9e085246fede",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3d64140271c4b04b52d6301cb25f7e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name='google/flan-t5-base'\n",
        "\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "VY3bl3dNwbad"
      },
      "source": [
        "É possível extrair o número de parâmetros do modelo e descobrir quantos deles são treináveis. A seguinte função pode ser usada para fazer isso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "I8TGVFwawbae",
        "outputId": "b718d9e0-690e-40c9-ef24-102a9d736dbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable model parameters: 247577856\n",
            "all model parameters: 247577856\n",
            "percentage of trainable model parameters: 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Define uma função para imprimir o número de parâmetros treináveis e não treináveis do modelo especificado\n",
        "def print_number_of_trainable_model_parameters(model):\n",
        "    # Inicializa contadores para o total de parâmetros treináveis e não treináveis\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "\n",
        "    # Itera sobre todos os parâmetros do modelo, somando o número total de parâmetros e contando os parâmetros treináveis\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()  # Adiciona o número total de parâmetros\n",
        "        if param.requires_grad:  # Verifica se o parâmetro requer gradiente (ou seja, é treinável)\n",
        "            trainable_model_params += param.numel()  # Se for treinável, adiciona ao contador de parâmetros treináveis\n",
        "\n",
        "    # Retorna uma string formatada contendo informações sobre os parâmetros do modelo\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "# Chama a função com o modelo original e imprime o resultado\n",
        "print(print_number_of_trainable_model_parameters(original_model))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "5AsE9EKLwbae"
      },
      "source": [
        "<a name='1.3'></a>\n",
        "### 1.3 - Teste do Modelo com Zero Shot Inferencia\n",
        "\n",
        "\n",
        "Teste o modelo com a inferência de zero shot. Você verá que o modelo tem dificuldade em resumir o diálogo em comparação com o resumo de referência, mas ele consegue extrair algumas informações importantes do texto, o que indica que o modelo pode ser ajustado para a tarefa em questão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "_qlF6AjVwbaf",
        "outputId": "6c6c859d-df18-4257-c1bf-261de178668a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "\n",
            "Summarize the following conversation.\n",
            "\n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "Summary:\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "#Person1#: I'm thinking of upgrading my computer.\n"
          ]
        }
      ],
      "source": [
        "# Define o índice do exemplo a ser utilizado\n",
        "index = 200\n",
        "\n",
        "# Extrai o diálogo e o resumo do conjunto de dados para o índice especificado\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "summary = dataset['test'][index]['summary']\n",
        "\n",
        "# Cria o prompt para a geração do resumo, incorporando o diálogo\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "# Tokeniza o prompt para que seja compreensível pelo modelo, retornando tensores PyTorch\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "# Gera a saída (resumo) utilizando o modelo original\n",
        "output = tokenizer.decode(\n",
        "    original_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "# Cria uma linha de traços para fins de formatação\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "\n",
        "# Imprime uma linha de traços seguida pelo prompt, resumo humano de referência, e o resumo gerado pelo modelo\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "2gLoxRoRwbag"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Performance do Fine-Tuning completo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "RhOUa3Howbah"
      },
      "source": [
        "<a name='2.1'></a>\n",
        "### 2.1 - Pré-processar o Conjunto de Dados Diálogo-Sumário\n",
        "\n",
        "Treine a sequência de diálogo-sumário (prompt-resposta) convertendo-a em instruções explícitas para o LLM. Adicione uma instrução ao início do diálogo com `Summarize the following conversation` e ao início do summary com `Summary` conforme segue:\n",
        "\n",
        "Prompt de Treinamento (diálogo):\n",
        "\n",
        "```\n",
        "Summarize the following conversation.\n",
        "\n",
        "    Chris: This is his part of the conversation.\n",
        "    Antje: This is her part of the conversation.\n",
        "    \n",
        "Summary:\n",
        "```\n",
        "\n",
        "Prompt de Treinamento (summary):\n",
        "```\n",
        "Both Chris and Antje participated in the conversation.\n",
        "```\n",
        "\n",
        "\n",
        "Em seguida, pré-processar o conjunto de dados de prompt-resposta em tokens e extrair seus `input_ids` (1 por token)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "639c5ab9edd741548a4b82e5afa0824c",
            "315aa2d6694f47e5be945ef70c8e6a05",
            "e4435c6c804f4cdcbd5f518f70e4d8ed"
          ]
        },
        "id": "QR2lTEnVwbah",
        "outputId": "c49054b2-73d5-4b88-c816-6500473fe294"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "639c5ab9edd741548a4b82e5afa0824c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "315aa2d6694f47e5be945ef70c8e6a05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4435c6c804f4cdcbd5f518f70e4d8ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define uma função para tokenizar os exemplos do conjunto de dados\n",
        "def tokenize_function(example):\n",
        "    # Define o início e o fim do prompt para a geração de resumo\n",
        "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
        "    end_prompt = '\\n\\nSummary: '\n",
        "\n",
        "    # Cria um prompt para cada diálogo no exemplo e adiciona o prompt de resumo\n",
        "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
        "\n",
        "    # Tokeniza os prompts e os resumos, adicionando 'input_ids' e 'labels' aos exemplos\n",
        "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    return example\n",
        "\n",
        "# O conjunto de dados na verdade contém 3 divisões diferentes: treino, validação, teste.\n",
        "# O código da função tokenize_function lida com todos os dados em todas as divisões em lotes.\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove as colunas 'id', 'topic', 'dialogue' e 'summary' dos conjuntos de dados tokenizados\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "wQgGsQajwbai"
      },
      "source": [
        "Para economizar tempo, vamos subamostrar o conjunto de dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "49a3fe1f4a2c4b888de2d2faf7c366d5",
            "27d8881f999b4a6d9bdb61eba250d995",
            "ebaf87974b054662a596c5448cda286a"
          ]
        },
        "id": "XIleFODcwbai",
        "outputId": "8476cb1d-5361-42b6-ad1d-654b92449ef5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49a3fe1f4a2c4b888de2d2faf7c366d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27d8881f999b4a6d9bdb61eba250d995",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebaf87974b054662a596c5448cda286a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Filtra os exemplos dos conjuntos de dados tokenizados mantendo apenas aqueles cujos índices são múltiplos de 100\n",
        "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "8SJ81fwOwbai"
      },
      "source": [
        "Verifique as formas de todas as três partes do conjunto de dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "PY0IfFZBwbaj",
        "outputId": "4163166d-b074-4cc3-c4d7-7872d9c36b81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes of the datasets:\n",
            "Training: (125, 2)\n",
            "Validation: (5, 2)\n",
            "Test: (15, 2)\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'labels'],\n",
            "        num_rows: 125\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'labels'],\n",
            "        num_rows: 5\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'labels'],\n",
            "        num_rows: 15\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Imprime os formatos dos conjuntos de dados\n",
        "print(f\"Shapes of the datasets:\")\n",
        "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
        "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
        "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
        "\n",
        "# Imprime os conjuntos de dados tokenizados\n",
        "print(tokenized_datasets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQAq57Jlwbak"
      },
      "source": [
        "A saida do dataset ja esta pronto para o fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGaWEraLwbak"
      },
      "source": [
        "<a name='2.2'></a>\n",
        "### 2.2 - Fine-Tuning do Modelo com o Conjunto de Dados Pré-processado\n",
        "\n",
        "Agora utilize a classe `Trainer` incorporada da Hugging Face (consulte a documentação [here](https://huggingface.co/docs/transformers/main_classes/trainer)). Passe o conjunto de dados pré-processado com referência ao modelo original. Outros parâmetros de treinamento são encontrados experimentalmente e não há necessidade de entrar em detalhes sobre eles no momento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "xTjLElrKwbak"
      },
      "outputs": [],
      "source": [
        "# Define o diretório de saída para salvar os resultados do treinamento, incluindo checkpoints e logs,\n",
        "# usando um nome baseado na data e hora atual\n",
        "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
        "\n",
        "# Define os argumentos de treinamento, incluindo o diretório de saída, a taxa de aprendizado,\n",
        "# o número de épocas de treinamento, o decaimento de peso, o intervalo de registro de logs e\n",
        "# o número máximo de passos (batches)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,  # Diretório de saída para salvar os resultados do treinamento\n",
        "    learning_rate=1e-5,     # Taxa de aprendizado\n",
        "    num_train_epochs=1,     # Número de épocas de treinamento\n",
        "    weight_decay=0.01,      # Decaimento de peso (regularização L2)\n",
        "    logging_steps=1,        # Intervalo de registro de logs durante o treinamento\n",
        "    max_steps=1             # Número máximo de passos (batches) de treinamento\n",
        ")\n",
        "\n",
        "# Define o treinador que será responsável por executar o treinamento do modelo\n",
        "trainer = Trainer(\n",
        "    model=original_model,                    # Modelo a ser treinado\n",
        "    args=training_args,                     # Argumentos de treinamento definidos anteriormente\n",
        "    train_dataset=tokenized_datasets['train'],   # Conjunto de dados de treinamento\n",
        "    eval_dataset=tokenized_datasets['validation'] # Conjunto de dados de validação\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Z67_Gd6jwbal",
        "outputId": "61d47ad6-673f-44a3-b21e-1546c7a97d13"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 00:00, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>49.250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1, training_loss=49.25, metrics={'train_runtime': 70.1219, 'train_samples_per_second': 0.114, 'train_steps_per_second': 0.014, 'total_flos': 5478058819584.0, 'train_loss': 49.25, 'epoch': 0.06})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inicia o processo de treinamento do modelo utilizando o treinador definido anteriormente\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRjGnS-Awbal"
      },
      "source": [
        "Treinar uma versão completamente ajustada do modelo levaria algumas horas em uma GPU. Para economizar tempo, faça o download de um ponto de verificação do modelo totalmente ajustado para usar no restante deste notebook. Este modelo totalmente ajustado também será referido como o **modelo instruído**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "gns3S6Onwbam",
        "outputId": "e97d55b7-0d2c-4882-8526-63961b16de9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "download: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/generation_config.json to flan-dialogue-summary-checkpoint/generation_config.json\n",
            "download: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/scheduler.pt to flan-dialogue-summary-checkpoint/scheduler.pt\n",
            "download: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/rng_state.pth to flan-dialogue-summary-checkpoint/rng_state.pth\n",
            "download: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/config.json to flan-dialogue-summary-checkpoint/config.json\n",
            "download: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/trainer_state.json to flan-dialogue-summary-checkpoint/trainer_state.json\n",
            "download: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/training_args.bin to flan-dialogue-summary-checkpoint/training_args.bin\n",
            "download: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/pytorch_model.bin to flan-dialogue-summary-checkpoint/pytorch_model.bin\n",
            "download: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/optimizer.pt to flan-dialogue-summary-checkpoint/optimizer.pt\n"
          ]
        }
      ],
      "source": [
        "# Copia recursivamente os arquivos do bucket S3 \"dlai-generative-ai/models/flan-dialogue-summary-checkpoint/\"\n",
        "# para o diretório local \"./flan-dialogue-summary-checkpoint/\"\n",
        "!aws s3 cp --recursive s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/ ./flan-dialogue-summary-checkpoint/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "z6NkLpbBwbam"
      },
      "source": [
        "O tamanho do modelo instruído baixado é de aproximadamente 1 GB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "PluCCMS-wbam",
        "outputId": "c276662f-a6e3-4666-e552-5e1da8dc7280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "-rw-r--r-- 1 root root 945M May 15  2023 ./flan-dialogue-summary-checkpoint/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "# Lista informações detalhadas do arquivo \"pytorch_model.bin\" no diretório \"./flan-dialogue-summary-checkpoint/\"\n",
        "!ls -alh ./flan-dialogue-summary-checkpoint/pytorch_model.bin\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "2XRVTRS-wban"
      },
      "source": [
        "Crie uma instância da classe `AutoModelForSeq2SeqLM` para o modelo instruído:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "53ucn3tPwbat"
      },
      "outputs": [],
      "source": [
        "# Carrega um modelo de geração de sequências de resumo a partir do diretório local \"./flan-dialogue-summary-checkpoint\"\n",
        "# usando os parâmetros pré-treinados encontrados neste diretório e especifica a precisão de ponto flutuante como bfloat16\n",
        "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./flan-dialogue-summary-checkpoint\", torch_dtype=torch.bfloat16)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSAjYbPhwbau"
      },
      "source": [
        "<a name='2.3'></a>\n",
        "### 2.3 - Avalie o Modelo Qualitativamente (Avaliação Humana)\n",
        "\n",
        "Como em muitas aplicações de IA generativa, uma abordagem qualitativa em que você se pergunta \"Meu modelo está se comportando como deveria?\" geralmente é um bom ponto de partida. No exemplo abaixo (o mesmo com o qual começamos este notebook), você pode ver como o modelo ajustado é capaz de criar um resumo razoável do diálogo em comparação com a incapacidade original de entender o que está sendo pedido ao modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4L8gCpeiwbau",
        "outputId": "6c839039-bfbb-4612-b872-7809d71b8a7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "ORIGINAL MODEL:\n",
            "#Person1#: You'd like to upgrade your computer. #Person2: You'd like to upgrade your computer.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "INSTRUCT MODEL:\n",
            "#Person1# suggests #Person2# upgrading #Person2#'s system, hardware, and CD-ROM drive. #Person2# thinks it's great.\n"
          ]
        }
      ],
      "source": [
        "# Define o índice do exemplo a ser utilizado\n",
        "index = 200\n",
        "\n",
        "# Extrai o diálogo e o resumo humano de referência do conjunto de dados para o índice especificado\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "human_baseline_summary = dataset['test'][index]['summary']\n",
        "\n",
        "# Cria um prompt para a geração de resumo, incorporando o diálogo\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "# Tokeniza o prompt para que seja compreensível pelos modelos, retornando tensores PyTorch\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Gera resumos utilizando o modelo original e o modelo de instrução\n",
        "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Imprime os resultados: resumo humano de referência, resumo gerado pelo modelo original e resumo gerado pelo modelo de instrução\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
        "print(dash_line)\n",
        "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
        "print(dash_line)\n",
        "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXJQPW7dwbav"
      },
      "source": [
        "<a name='2.4'></a>\n",
        "### 2.4 - Avalie o Modelo Quantitativamente (com a Métrica ROUGE)\n",
        "\n",
        "A [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) ajuda a quantificar a validade das sumarizações produzidas pelos modelos. Ela compara as sumarizações com um resumo \"baseline\" que geralmente é criado por um humano. Embora não seja perfeita, ela indica o aumento geral na eficácia de sumarização que conseguimos alcançar através do fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "14ef023de9854102af948951d1861eab"
          ]
        },
        "id": "nVTIDz2Nwbav",
        "outputId": "577e89fd-aa87-4983-8abc-f539c1b08a73"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14ef023de9854102af948951d1861eab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Carrega o módulo de avaliação para ROUGE (ROUGE-N, ROUGE-L, ROUGE-W) usando a biblioteca evaluate\n",
        "rouge = evaluate.load('rouge')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kzqg211Zwbav"
      },
      "source": [
        "Gere as saídas para a amostra do conjunto de dados de teste (apenas 10 diálogos e sumários para economizar tempo) e salve os resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "uHaDIM7ewbaw",
        "outputId": "9cb6d370-80a4-4afc-ee84-079165623169"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>human_baseline_summaries</th>\n",
              "      <th>original_model_summaries</th>\n",
              "      <th>instruct_model_summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
              "      <td>#Person1#: Thank you for your time.</td>\n",
              "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In order to prevent employees from wasting tim...</td>\n",
              "      <td>This memo should go out as an intra-office mem...</td>\n",
              "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
              "      <td>Employees who use the Instant Messaging progra...</td>\n",
              "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#Person2# arrives late because of traffic jam....</td>\n",
              "      <td>#Person1: I'm sorry you're stuck in traffic. #...</td>\n",
              "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
              "      <td>#Person1#: I'm finally here. I've got a traffi...</td>\n",
              "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
              "      <td>The driver of the car is stuck in a traffic jam.</td>\n",
              "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
              "      <td>Masha and Hero are getting divorced.</td>\n",
              "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
              "      <td>Masha and Hero are getting married.</td>\n",
              "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
              "      <td>Masha and Hero are getting divorced.</td>\n",
              "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
              "      <td>#Person1#: Happy birthday, Brian. #Person2#: H...</td>\n",
              "      <td>Brian's birthday is coming. #Person1# invites ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            human_baseline_summaries  \\\n",
              "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
              "1  In order to prevent employees from wasting tim...   \n",
              "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
              "3  #Person2# arrives late because of traffic jam....   \n",
              "4  #Person2# decides to follow #Person1#'s sugges...   \n",
              "5  #Person2# complains to #Person1# about the tra...   \n",
              "6  #Person1# tells Kate that Masha and Hero get d...   \n",
              "7  #Person1# tells Kate that Masha and Hero are g...   \n",
              "8  #Person1# and Kate talk about the divorce betw...   \n",
              "9  #Person1# and Brian are at the birthday party ...   \n",
              "\n",
              "                            original_model_summaries  \\\n",
              "0                #Person1#: Thank you for your time.   \n",
              "1  This memo should go out as an intra-office mem...   \n",
              "2  Employees who use the Instant Messaging progra...   \n",
              "3  #Person1: I'm sorry you're stuck in traffic. #...   \n",
              "4  #Person1#: I'm finally here. I've got a traffi...   \n",
              "5   The driver of the car is stuck in a traffic jam.   \n",
              "6               Masha and Hero are getting divorced.   \n",
              "7                Masha and Hero are getting married.   \n",
              "8               Masha and Hero are getting divorced.   \n",
              "9  #Person1#: Happy birthday, Brian. #Person2#: H...   \n",
              "\n",
              "                            instruct_model_summaries  \n",
              "0  #Person1# asks Ms. Dawson to take a dictation ...  \n",
              "1  #Person1# asks Ms. Dawson to take a dictation ...  \n",
              "2  #Person1# asks Ms. Dawson to take a dictation ...  \n",
              "3  #Person2# got stuck in traffic again. #Person1...  \n",
              "4  #Person2# got stuck in traffic again. #Person1...  \n",
              "5  #Person2# got stuck in traffic again. #Person1...  \n",
              "6  Masha and Hero are getting divorced. Kate can'...  \n",
              "7  Masha and Hero are getting divorced. Kate can'...  \n",
              "8  Masha and Hero are getting divorced. Kate can'...  \n",
              "9  Brian's birthday is coming. #Person1# invites ...  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Extrai os diálogos e resumos humanos de referência para os primeiros 10 exemplos do conjunto de dados de teste\n",
        "dialogues = dataset['test'][0:10]['dialogue']\n",
        "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
        "\n",
        "# Inicializa listas para armazenar os resumos gerados pelos modelos original e de instrução\n",
        "original_model_summaries = []\n",
        "instruct_model_summaries = []\n",
        "\n",
        "# Itera sobre cada diálogo para gerar resumos usando os modelos original e de instrução\n",
        "for _, dialogue in enumerate(dialogues):\n",
        "    prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary: \"\"\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    # Gera resumo usando o modelo original\n",
        "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "    original_model_summaries.append(original_model_text_output)\n",
        "\n",
        "    # Gera resumo usando o modelo de instrução\n",
        "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "    instruct_model_summaries.append(instruct_model_text_output)\n",
        "\n",
        "# Agrupa os resumos em uma lista de tuplas\n",
        "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
        "\n",
        "# Cria um DataFrame pandas com os resumos gerados e os resumos humanos de referência\n",
        "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "6FEY34TJwbaw"
      },
      "source": [
        "\n",
        "Avalie os modelos calculando as métricas ROUGE. Observe a melhoria nos resultados!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "GrVVrPngwbax",
        "outputId": "4493902d-ad36-4956-adab-d88874b62d40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ORIGINAL MODEL:\n",
            "{'rouge1': 0.24223171760013867, 'rouge2': 0.10614243734192583, 'rougeL': 0.21380459196706333, 'rougeLsum': 0.21740921541379205}\n",
            "INSTRUCT MODEL:\n",
            "{'rouge1': 0.41026607717457186, 'rouge2': 0.17840645241958838, 'rougeL': 0.2977022096267017, 'rougeLsum': 0.2987374187518165}\n"
          ]
        }
      ],
      "source": [
        "# Calcula as métricas ROUGE (ROUGE-N, ROUGE-L, ROUGE-W) para os resumos gerados pelo modelo original\n",
        "# usando os resumos humanos de referência correspondentes como referência\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "# Calcula as métricas ROUGE (ROUGE-N, ROUGE-L, ROUGE-W) para os resumos gerados pelo modelo de instrução\n",
        "# usando os resumos humanos de referência correspondentes como referência\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=instruct_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "# Imprime os resultados do modelo original\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "\n",
        "# Imprime os resultados do modelo de instrução\n",
        "print('INSTRUCT MODEL:')\n",
        "print(instruct_model_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x0bCLc6wbax"
      },
      "source": [
        "O arquivo `data/dialogue-summary-training-results.csv` contém uma lista pré-populada de todos os resultados do modelo que você pode usar para avaliar em uma seção maior de dados. Vamos fazer isso para cada um dos modelos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "mYRxRIBQwbay",
        "outputId": "7ff2da9c-1df0-46d4-dc9a-5fcb1a64f2b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ORIGINAL MODEL:\n",
            "{'rouge1': 0.2334158581572823, 'rouge2': 0.07603964187010573, 'rougeL': 0.20145520923859048, 'rougeLsum': 0.20145899339006135}\n",
            "INSTRUCT MODEL:\n",
            "{'rouge1': 0.42161291557556113, 'rouge2': 0.18035380596301792, 'rougeL': 0.3384439349963909, 'rougeLsum': 0.33835653595561666}\n"
          ]
        }
      ],
      "source": [
        "# Lê os resultados do arquivo CSV \"data/dialogue-summary-training-results.csv\" usando pandas\n",
        "results = pd.read_csv(\"data/dialogue-summary-training-results.csv\")\n",
        "\n",
        "# Extrai os resumos humanos de referência, os resumos gerados pelo modelo original e os resumos gerados pelo modelo de instrução\n",
        "human_baseline_summaries = results['human_baseline_summaries'].values\n",
        "original_model_summaries = results['original_model_summaries'].values\n",
        "instruct_model_summaries = results['instruct_model_summaries'].values\n",
        "\n",
        "# Calcula as métricas ROUGE (ROUGE-N, ROUGE-L, ROUGE-W) para os resumos gerados pelo modelo original\n",
        "# usando os resumos humanos de referência correspondentes como referência\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "# Calcula as métricas ROUGE (ROUGE-N, ROUGE-L, ROUGE-W) para os resumos gerados pelo modelo de instrução\n",
        "# usando os resumos humanos de referência correspondentes como referência\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=instruct_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "# Imprime os resultados do modelo original\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "\n",
        "# Imprime os resultados do modelo de instrução\n",
        "print('INSTRUCT MODEL:')\n",
        "print(instruct_model_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "LcFEsLe3wbay"
      },
      "source": [
        "Os resultados mostram uma melhoria substancial em todas as métricas ROUGE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "pJ-ks617wbay",
        "outputId": "abea1e08-20b9-4f00-fa19-065b68db94fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\n",
            "rouge1: 18.82%\n",
            "rouge2: 10.43%\n",
            "rougeL: 13.70%\n",
            "rougeLsum: 13.69%\n"
          ]
        }
      ],
      "source": [
        "# Imprime o título para indicar que os resultados exibidos são a melhoria percentual absoluta do MODELO DE INSTRUÇÃO sobre o MODELO ORIGINAL\n",
        "print(\"Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\")\n",
        "\n",
        "# Calcula a melhoria percentual absoluta do MODELO DE INSTRUÇÃO sobre o MODELO ORIGINAL para cada métrica ROUGE\n",
        "improvement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\n",
        "for key, value in zip(instruct_model_results.keys(), improvement):\n",
        "    # Imprime a métrica ROUGE e sua melhoria percentual absoluta\n",
        "    print(f'{key}: {value*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfJ9CfGfwbaz"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3 -  Fine-Tuning Eficiente de Parâmetros (PEFT).\n",
        "\n",
        "Agora, vamos realizar o **Fine-Tuning Eficiente de Parâmetros (PEFT)** em vez do \"fine-tuning completo\" como feito anteriormente. O PEFT é uma forma de fine-tuning instrucional que é muito mais eficiente do que o fine-tuning completo - com resultados de avaliação comparáveis, como você verá em breve.\n",
        "\n",
        "O PEFT é um termo genérico que inclui a **Adaptação de Baixa Ordem (LoRA)** e a sintonia de prompt (que NÃO É A MESMA que a engenharia de prompt!). Na maioria dos casos, quando alguém menciona PEFT, eles geralmente estão se referindo ao LoRA. LoRA, em um nível muito alto, permite ao usuário ajustar seu modelo usando menos recursos computacionais (em alguns casos, uma única GPU). Após o ajuste fino para uma tarefa específica, caso de uso ou inquilino com LoRA, o resultado é que o LLM original permanece inalterado e um \"adaptador LoRA\" recém-treinado surge. Este adaptador LoRA é muito, muito menor que o LLM original - da ordem de % de dígitos únicos do tamanho original do LLM (MBs versus GBs).\n",
        "\n",
        "Dito isso, no momento da inferência, o adaptador LoRA precisa ser reunido e combinado com seu LLM original para atender à solicitação de inferência. O benefício, no entanto, é que muitos adaptadores LoRA podem reutilizar o LLM original, o que reduz os requisitos de memória globais ao atender a várias tarefas e casos de uso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCxmaOLvwbaz"
      },
      "source": [
        "<a name='3.1'></a>\n",
        "### 3.1 - Configure o modelo PEFT/LoRA para Fine-Tuning.\n",
        "\n",
        "Você precisa configurar o modelo PEFT/LoRA para o fine-tuning com uma nova camada/adaptador de parâmetros. Usando o PEFT/LoRA, você está congelando o LLM subjacente e treinando apenas o adaptador. Dê uma olhada na configuração do LoRA abaixo. Observe o hiperparâmetro de rank (`r`), que define o rank/dimensão do adaptador a ser treinado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "sxvSn-h_wba0"
      },
      "outputs": [],
      "source": [
        "# Importa as bibliotecas necessárias do pacote peft\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# Define a configuração do modelo LORA (Lightweight Optimized Rank Adaptation)\n",
        "lora_config = LoraConfig(\n",
        "    r=32,                       # Rank\n",
        "    lora_alpha=32,              # Hiperparâmetro alpha do LORA\n",
        "    target_modules=[\"q\", \"v\"],  # Módulos-alvo para aplicar a adaptação de rank\n",
        "    lora_dropout=0.05,          # Taxa de dropout do LORA\n",
        "    bias=\"none\",                # Tipo de viés (bias)\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM  # Tipo de tarefa (neste caso, FLAN-T5, uma tarefa de modelo de linguagem seq2seq)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "HS_j0t1Qwba0"
      },
      "source": [
        "Adicione camadas/adaptadores LoRA aos parâmetros originais do LLM a serem treinados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "q5ZmzLh2wba0",
        "outputId": "5a5c33f5-e42d-4ae3-a990-e06cfa9b28a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable model parameters: 3538944\n",
            "all model parameters: 251116800\n",
            "percentage of trainable model parameters: 1.41%\n"
          ]
        }
      ],
      "source": [
        "# Obtém o modelo adaptado usando o método get_peft_model do pacote peft, fornecendo o modelo original e a configuração do LORA\n",
        "peft_model = get_peft_model(original_model, lora_config)\n",
        "\n",
        "# Imprime o número de parâmetros treináveis do modelo adaptado usando a função print_number_of_trainable_model_parameters\n",
        "print(print_number_of_trainable_model_parameters(peft_model))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "iUo14AE5wba1"
      },
      "source": [
        "<a name='3.2'></a>\n",
        "### 3.2 - Treine o Adaptador PEFT.\n",
        "\n",
        "Defina os argumentos de treinamento e crie uma instância do Trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "7s6nwNtnwba2"
      },
      "outputs": [],
      "source": [
        "# Define o diretório de saída para salvar os resultados do treinamento do modelo PEFT\n",
        "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
        "\n",
        "# Define os argumentos de treinamento para o modelo PEFT, incluindo o diretório de saída,\n",
        "# a taxa de aprendizado, o número de épocas de treinamento, o intervalo de registro de logs e\n",
        "# o número máximo de passos (batches)\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,          # Diretório de saída para salvar os resultados do treinamento\n",
        "    auto_find_batch_size=True,      # Encontrar automaticamente o tamanho do lote ideal\n",
        "    learning_rate=1e-3,             # Taxa de aprendizado (mais alta do que o ajuste fino completo)\n",
        "    num_train_epochs=1,             # Número de épocas de treinamento\n",
        "    logging_steps=1,                # Intervalo de registro de logs durante o treinamento\n",
        "    max_steps=1                     # Número máximo de passos (batches) de treinamento\n",
        ")\n",
        "\n",
        "# Define o treinador para o modelo PEFT, que será responsável por executar o treinamento do modelo\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,               # Modelo PEFT a ser treinado\n",
        "    args=peft_training_args,       # Argumentos de treinamento definidos anteriormente\n",
        "    train_dataset=tokenized_datasets[\"train\"],  # Conjunto de dados de treinamento\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "hH0wYRwtwba4",
        "outputId": "cfcc5b9e-25a7-484d-af9d-11ca590b0faf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 00:00, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>51.250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('./peft-dialogue-summary-checkpoint-local/tokenizer_config.json',\n",
              " './peft-dialogue-summary-checkpoint-local/special_tokens_map.json',\n",
              " './peft-dialogue-summary-checkpoint-local/tokenizer.json')"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inicia o processo de treinamento do modelo PEFT\n",
        "peft_trainer.train()\n",
        "\n",
        "# Define o caminho para salvar o modelo PEFT treinado e o tokenizador\n",
        "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
        "\n",
        "# Salva o modelo PEFT treinado no diretório especificado\n",
        "peft_trainer.model.save_pretrained(peft_model_path)\n",
        "\n",
        "# Salva o tokenizador no mesmo diretório\n",
        "tokenizer.save_pretrained(peft_model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "om_f3o-Owba5"
      },
      "source": [
        "O treinamento foi realizado em um subconjunto de dados. Para carregar um modelo PEFT totalmente treinado, leia um ponto de verificação de um modelo PEFT do S3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "tLTZQGL2wba5",
        "outputId": "d6d171b8-35ff-41d4-c894-7e47c4d3b0df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/tokenizer_config.json to peft-dialogue-summary-checkpoint-from-s3/tokenizer_config.json\n",
            "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/adapter_config.json to peft-dialogue-summary-checkpoint-from-s3/adapter_config.json\n",
            "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/special_tokens_map.json to peft-dialogue-summary-checkpoint-from-s3/special_tokens_map.json\n",
            "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/adapter_model.bin to peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin\n",
            "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/tokenizer.json to peft-dialogue-summary-checkpoint-from-s3/tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "!aws s3 cp --recursive s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/ ./peft-dialogue-summary-checkpoint-from-s3/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "LnHvtKRfwba6"
      },
      "source": [
        "Verifique se o tamanho deste modelo é muito menor do que o do LLM original:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "qgqez7sewba6",
        "outputId": "e5010dd1-5805-4e28-fa62-d29883ca6bf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "-rw-r--r-- 1 root root 14208525 May 15  2023 ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin\n"
          ]
        }
      ],
      "source": [
        "!ls -al ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "L7UrWBCewba6"
      },
      "source": [
        "Prepare este modelo adicionando um adaptador ao modelo original FLAN-T5. Você está definindo `is_trainable=False` porque o plano é apenas realizar inferência com este modelo PEFT. Se você estivesse preparando o modelo para mais treinamento, você definiria `is_trainable=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "9gjd2kb7wba7"
      },
      "outputs": [],
      "source": [
        "# Importa as classes necessárias do pacote peft\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "# Carrega o modelo base FLAN-T5 e o tokenizador associado\n",
        "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "# Carrega o modelo PEFT especificando o modelo base, o diretório de checkpoints, o tipo de tensor e se o modelo é treinável\n",
        "peft_model = PeftModel.from_pretrained(peft_model_base,\n",
        "                                       './peft-dialogue-summary-checkpoint-from-s3/',\n",
        "                                       torch_dtype=torch.bfloat16,\n",
        "                                       is_trainable=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "QeomEj5Rwba7"
      },
      "source": [
        "O número de parâmetros treináveis será `0` devido à configuração `is_trainable=False`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "1zPGYzydwba7",
        "outputId": "62444396-867d-499c-bce4-6cece1d0d27b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable model parameters: 0\n",
            "all model parameters: 251116800\n",
            "percentage of trainable model parameters: 0.00%\n"
          ]
        }
      ],
      "source": [
        "# Imprime o número de parâmetros treináveis do modelo PEFT\n",
        "print(print_number_of_trainable_model_parameters(peft_model))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHqBh-7rwba8"
      },
      "source": [
        "<a name='3.3'></a>\n",
        "### 3.3 - Avalie o Modelo Qualitativamente (Avaliação Humana)\n",
        "\n",
        "Faça inferências para o mesmo exemplo das seções [1.3](#1.3) e [2.3](#2.3), com o modelo original, totalmente ajustado e o modelo PEFT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "TsO8951mwba8",
        "outputId": "f0f334c2-68ab-48ae-fdaf-1246c0a23842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "ORIGINAL MODEL:\n",
            "#Pork1: Have you considered upgrading your system? #Person1: Yes, but I'd like to make some improvements. #Pork1: I'd like to make a painting program. #Person1: I'd like to make a flyer. #Person1: I'd like to make a banner. #Person2: I'd like to make a banner. #Person1: I'd like to make a fly. #Person2: I'd like to make a CD-ROM drive. #Person1: I'd like to make a CD-ROM drive.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "INSTRUCT MODEL:\n",
            "#Person1# suggests #Person2# upgrading #Person2#'s system, hardware, and CD-ROM drive. #Person2# thinks it's great.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "PEFT MODEL: #Person1# recommends adding a painting program to #Person2#'s software and upgrading hardware. #Person2# also wants to upgrade the hardware because it's outdated now.\n"
          ]
        }
      ],
      "source": [
        "# Define o índice do diálogo a ser resumido\n",
        "index = 200\n",
        "\n",
        "# Obtém o diálogo e o resumo humano de referência correspondente do conjunto de dados de teste\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "baseline_human_summary = dataset['test'][index]['summary']\n",
        "\n",
        "# Monta o prompt para resumir o diálogo\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary: \"\"\"\n",
        "\n",
        "# Converte o prompt em IDs de tokens usando o tokenizador\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Gera o resumo usando o modelo original\n",
        "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Gera o resumo usando o modelo de instrução\n",
        "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Gera o resumo usando o modelo PEFT\n",
        "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Imprime os resumos gerados e o resumo humano de referência\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
        "print(dash_line)\n",
        "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
        "print(dash_line)\n",
        "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
        "print(dash_line)\n",
        "print(f'PEFT MODEL: {peft_model_text_output}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygmwLoowba8"
      },
      "source": [
        "<a name='3.4'></a>\n",
        "### 3.4 - Avalie o Modelo Quantitativamente (com a Métrica ROUGE)\n",
        "Realize inferências para a amostra do conjunto de dados de teste (apenas 10 diálogos e sumários para economizar tempo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "CyCoaWJewba9",
        "outputId": "15b7ba19-5db0-4090-fe48-4e2b7a424da5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>human_baseline_summaries</th>\n",
              "      <th>original_model_summaries</th>\n",
              "      <th>instruct_model_summaries</th>\n",
              "      <th>peft_model_summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
              "      <td>This is a memo to all employees.</td>\n",
              "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
              "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In order to prevent employees from wasting tim...</td>\n",
              "      <td>The memo is to be distributed to all employees...</td>\n",
              "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
              "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
              "      <td>#Person1#: This is an intra-office memo. #Pers...</td>\n",
              "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
              "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#Person2# arrives late because of traffic jam....</td>\n",
              "      <td>The traffic jam in the Carrefour area is a pro...</td>\n",
              "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
              "      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
              "      <td>#Person1: I'm finally here! #Person2: I'm fina...</td>\n",
              "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
              "      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
              "      <td>People are complaining about the way the car i...</td>\n",
              "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
              "      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
              "      <td>Masha and Hero are getting divorced.</td>\n",
              "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
              "      <td>Kate tells #Person2# Masha and Hero are gettin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
              "      <td>Masha and Hero are getting divorced.</td>\n",
              "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
              "      <td>Kate tells #Person2# Masha and Hero are gettin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
              "      <td>The divorce is being finalized.</td>\n",
              "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
              "      <td>Kate tells #Person2# Masha and Hero are gettin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
              "      <td>Brian's birthday party was a great success.</td>\n",
              "      <td>Brian's birthday is coming. #Person1# invites ...</td>\n",
              "      <td>Brian remembers his birthday and invites #Pers...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            human_baseline_summaries  \\\n",
              "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
              "1  In order to prevent employees from wasting tim...   \n",
              "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
              "3  #Person2# arrives late because of traffic jam....   \n",
              "4  #Person2# decides to follow #Person1#'s sugges...   \n",
              "5  #Person2# complains to #Person1# about the tra...   \n",
              "6  #Person1# tells Kate that Masha and Hero get d...   \n",
              "7  #Person1# tells Kate that Masha and Hero are g...   \n",
              "8  #Person1# and Kate talk about the divorce betw...   \n",
              "9  #Person1# and Brian are at the birthday party ...   \n",
              "\n",
              "                            original_model_summaries  \\\n",
              "0                   This is a memo to all employees.   \n",
              "1  The memo is to be distributed to all employees...   \n",
              "2  #Person1#: This is an intra-office memo. #Pers...   \n",
              "3  The traffic jam in the Carrefour area is a pro...   \n",
              "4  #Person1: I'm finally here! #Person2: I'm fina...   \n",
              "5  People are complaining about the way the car i...   \n",
              "6               Masha and Hero are getting divorced.   \n",
              "7               Masha and Hero are getting divorced.   \n",
              "8                    The divorce is being finalized.   \n",
              "9        Brian's birthday party was a great success.   \n",
              "\n",
              "                            instruct_model_summaries  \\\n",
              "0  #Person1# asks Ms. Dawson to take a dictation ...   \n",
              "1  #Person1# asks Ms. Dawson to take a dictation ...   \n",
              "2  #Person1# asks Ms. Dawson to take a dictation ...   \n",
              "3  #Person2# got stuck in traffic again. #Person1...   \n",
              "4  #Person2# got stuck in traffic again. #Person1...   \n",
              "5  #Person2# got stuck in traffic again. #Person1...   \n",
              "6  Masha and Hero are getting divorced. Kate can'...   \n",
              "7  Masha and Hero are getting divorced. Kate can'...   \n",
              "8  Masha and Hero are getting divorced. Kate can'...   \n",
              "9  Brian's birthday is coming. #Person1# invites ...   \n",
              "\n",
              "                                peft_model_summaries  \n",
              "0  #Person1# asks Ms. Dawson to take a dictation ...  \n",
              "1  #Person1# asks Ms. Dawson to take a dictation ...  \n",
              "2  #Person1# asks Ms. Dawson to take a dictation ...  \n",
              "3  #Person2# got stuck in traffic and #Person1# s...  \n",
              "4  #Person2# got stuck in traffic and #Person1# s...  \n",
              "5  #Person2# got stuck in traffic and #Person1# s...  \n",
              "6  Kate tells #Person2# Masha and Hero are gettin...  \n",
              "7  Kate tells #Person2# Masha and Hero are gettin...  \n",
              "8  Kate tells #Person2# Masha and Hero are gettin...  \n",
              "9  Brian remembers his birthday and invites #Pers...  "
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Obtém os diálogos e os resumos humanos de referência dos primeiros 10 exemplos do conjunto de dados de teste\n",
        "dialogues = dataset['test'][0:10]['dialogue']\n",
        "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
        "\n",
        "# Inicializa listas vazias para armazenar os resumos gerados pelos modelos original, de instrução e PEFT\n",
        "original_model_summaries = []\n",
        "instruct_model_summaries = []\n",
        "peft_model_summaries = []\n",
        "\n",
        "# Itera sobre os diálogos e seus índices no conjunto de dados\n",
        "for idx, dialogue in enumerate(dialogues):\n",
        "    # Monta o prompt para resumir o diálogo\n",
        "    prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary: \"\"\"\n",
        "\n",
        "    # Converte o prompt em IDs de tokens usando o tokenizador\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    # Obtém o resumo humano de referência correspondente ao índice atual\n",
        "    human_baseline_text_output = human_baseline_summaries[idx]\n",
        "\n",
        "    # Gera o resumo usando o modelo original\n",
        "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Gera o resumo usando o modelo de instrução\n",
        "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Gera o resumo usando o modelo PEFT\n",
        "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Adiciona os resumos gerados pelas três abordagens aos respectivos listas\n",
        "    original_model_summaries.append(original_model_text_output)\n",
        "    instruct_model_summaries.append(instruct_model_text_output)\n",
        "    peft_model_summaries.append(peft_model_text_output)\n",
        "\n",
        "# Combina os resumos humanos de referência e os resumos gerados pelos modelos em uma lista de tuplas\n",
        "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n",
        "\n",
        "# Cria um DataFrame a partir das tuplas, nomeando as colunas\n",
        "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\n",
        "\n",
        "# Retorna o DataFrame contendo os resumos gerados pelos modelos e os resumos humanos de referência\n",
        "df\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "_Nz19ZPwwba9"
      },
      "source": [
        "Calcule o escore ROUGE para este subconjunto de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "1SqBiTCJwba9",
        "outputId": "94c53b0f-6b56-4eda-b57d-ba67cfd56e94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ORIGINAL MODEL:\n",
            "{'rouge1': 0.26528933794361415, 'rouge2': 0.11408233383308347, 'rougeL': 0.2496716571243882, 'rougeLsum': 0.2542598261388923}\n",
            "INSTRUCT MODEL:\n",
            "{'rouge1': 0.41026607717457186, 'rouge2': 0.17840645241958838, 'rougeL': 0.2977022096267017, 'rougeLsum': 0.2987374187518165}\n",
            "PEFT MODEL:\n",
            "{'rouge1': 0.3725351062275605, 'rouge2': 0.12138811933618107, 'rougeL': 0.27620639623170606, 'rougeLsum': 0.2758134870822362}\n"
          ]
        }
      ],
      "source": [
        "# Carrega o módulo de avaliação ROUGE\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "# Calcula as métricas ROUGE para os resumos gerados pelo modelo original\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "# Calcula as métricas ROUGE para os resumos gerados pelo modelo de instrução\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=instruct_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "# Calcula as métricas ROUGE para os resumos gerados pelo modelo PEFT\n",
        "peft_model_results = rouge.compute(\n",
        "    predictions=peft_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "# Imprime os resultados de ROUGE para cada modelo\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('INSTRUCT MODEL:')\n",
        "print(instruct_model_results)\n",
        "print('PEFT MODEL:')\n",
        "print(peft_model_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcGbn4_rwba-"
      },
      "source": [
        "Observe que os resultados do modelo PEFT não são tão ruins, enquanto o processo de treinamento foi muito mais fácil!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdIBjn2iwba-"
      },
      "source": [
        "Você já calculou o escore ROUGE no conjunto de dados completo, após carregar os resultados do arquivo `data/dialogue-summary-training-results.csv`. Carregue os valores para o modelo PEFT agora e verifique seu desempenho em comparação com outros modelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "53WjqbeYwba-",
        "outputId": "61da97b4-4d96-4907-b5c6-72bb511bdf1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ORIGINAL MODEL:\n",
            "{'rouge1': 0.2334158581572823, 'rouge2': 0.07603964187010573, 'rougeL': 0.20145520923859048, 'rougeLsum': 0.20145899339006135}\n",
            "INSTRUCT MODEL:\n",
            "{'rouge1': 0.42161291557556113, 'rouge2': 0.18035380596301792, 'rougeL': 0.3384439349963909, 'rougeLsum': 0.33835653595561666}\n",
            "PEFT MODEL:\n",
            "{'rouge1': 0.40810631575616746, 'rouge2': 0.1633255794568712, 'rougeL': 0.32507074586565354, 'rougeLsum': 0.3248950182867091}\n"
          ]
        }
      ],
      "source": [
        "# Obtém os resumos gerados pelos modelos e os resumos humanos de referência do DataFrame de resultados\n",
        "human_baseline_summaries = results['human_baseline_summaries'].values\n",
        "original_model_summaries = results['original_model_summaries'].values\n",
        "instruct_model_summaries = results['instruct_model_summaries'].values\n",
        "peft_model_summaries     = results['peft_model_summaries'].values\n",
        "\n",
        "# Calcula as métricas ROUGE para os resumos gerados pelo modelo original\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "# Calcula as métricas ROUGE para os resumos gerados pelo modelo de instrução\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=instruct_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "# Calcula as métricas ROUGE para os resumos gerados pelo modelo PEFT\n",
        "peft_model_results = rouge.compute(\n",
        "    predictions=peft_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "# Imprime os resultados de ROUGE para cada modelo\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('INSTRUCT MODEL:')\n",
        "print(instruct_model_results)\n",
        "print('PEFT MODEL:')\n",
        "print(peft_model_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RElhU69wba_"
      },
      "source": [
        "Os resultados mostram menos melhoria em relação ao fine-tuning completo, mas os benefícios do PEFT geralmente superam as métricas de desempenho ligeiramente mais baixas.\n",
        "\n",
        "Calcule a melhoria do PEFT em relação ao modelo original:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4q_VFV1Gwba_",
        "outputId": "76fc5288-0a4b-4452-eb42-501e38a6470a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\n",
            "rouge1: 17.47%\n",
            "rouge2: 8.73%\n",
            "rougeL: 12.36%\n",
            "rougeLsum: 12.34%\n"
          ]
        }
      ],
      "source": [
        "# Imprime a melhoria percentual absoluta do modelo PEFT em relação ao modelo original\n",
        "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
        "\n",
        "# Calcula a diferença entre as métricas ROUGE do modelo PEFT e do modelo original\n",
        "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
        "\n",
        "# Itera sobre as métricas ROUGE e suas melhorias percentuais absolutas\n",
        "for key, value in zip(peft_model_results.keys(), improvement):\n",
        "    print(f'{key}: {value*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0AD2qbEwbbA"
      },
      "source": [
        "Agora, calcule a melhoria do PEFT em relação a um modelo totalmente ajustado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "xsMawKtywbbB",
        "outputId": "cc7ba63a-74d4-4416-ac99-764b320be507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\n",
            "rouge1: -1.35%\n",
            "rouge2: -1.70%\n",
            "rougeL: -1.34%\n",
            "rougeLsum: -1.35%\n"
          ]
        }
      ],
      "source": [
        "# Imprime a melhoria percentual absoluta do modelo PEFT em relação ao modelo de instrução\n",
        "print(\"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n",
        "\n",
        "# Calcula a diferença entre as métricas ROUGE do modelo PEFT e do modelo de instrução\n",
        "improvement = (np.array(list(peft_model_results.values())) - np.array(list(instruct_model_results.values())))\n",
        "\n",
        "# Itera sobre as métricas ROUGE e suas melhorias percentuais absolutas\n",
        "for key, value in zip(peft_model_results.keys(), improvement):\n",
        "    print(f'{key}: {value*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpZ62b2DwbbB"
      },
      "source": [
        "Aqui você vê uma pequena diminuição percentual nas métricas ROUGE em comparação com o fine-tuning completo. No entanto, o treinamento requer muito menos recursos computacionais e de memória (geralmente apenas uma única GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjqZh-6fwbbH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 20,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": true,
        "memoryGiB": 0,
        "name": "ml.geospatial.interactive",
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "vcpuNum": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 57,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.trn1.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 58,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1.32xlarge",
        "vcpuNum": 128
      },
      {
        "_defaultOrder": 59,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1n.32xlarge",
        "vcpuNum": 128
      }
    ],
    "colab": {
      "provenance": []
    },
    "instance_type": "ml.m5.2xlarge",
    "kernelspec": {
      "display_name": "Python 3 (Data Science 3.0)",
      "language": "python",
      "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}